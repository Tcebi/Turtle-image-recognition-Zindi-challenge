{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d84df6a-afe3-4ce7-82ad-a1a3c213dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "import tensorflow as tf\n",
    "import image_modeling   # import image_modeling.py file\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db312e4b-870b-4642-a1e7-0608f339f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv-data\n",
    "image_dir = \"../images/\"\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.image_id = train_data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.image_id = test_data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "\n",
    "extra_data = pd.read_csv('../data/extra_images_loc.csv')\n",
    "extra_data.image_id = extra_data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "\n",
    "#Get unique_turtle_ids from train.csv\n",
    "unique_turtle_ids = list(extra_data['turtle_id'].unique())\n",
    "#Add category for new turtle for test set\n",
    "unique_turtle_ids.append(\"new_turtle\")\n",
    "#Get number of images for train/test split\n",
    "split = 0.75\n",
    "lines = round(len(extra_data)*split)\n",
    "length_data = len(extra_data)\n",
    "\n",
    "#We set some parameters for the model\n",
    "HEIGHT = 224 #image height\n",
    "WIDTH = 224 #image width\n",
    "CHANNELS = 3 #image RGB channels\n",
    "CLASS_NAMES = list(extra_data['image_location'].unique())\n",
    "NCLASSES = len(CLASS_NAMES)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 10 * BATCH_SIZE\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "TRAINING_SIZE = lines\n",
    "VALIDATION_SIZE = length_data - lines                    \n",
    "VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1944dcc6-7e39-4b8b-8efb-708f0942fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(augment = True):\n",
    "    if augment == True:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                rotation_range     = 40,\n",
    "                width_shift_range  = 0.2,\n",
    "                height_shift_range = 0.2,\n",
    "                rescale            = 1./255,\n",
    "                shear_range        = 0.2,\n",
    "                zoom_range         = 0.2,\n",
    "                horizontal_flip    = True,\n",
    "                fill_mode          = 'nearest')\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "    return train_datagen, test_datagen\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "#x_col value : which will be the name of column(in dataframe) having file names\n",
    "#y_col value : which will be the name of column(in dataframe) having class/label\n",
    "\n",
    "def use_image_generator(train_datagen, test_datagen, training=True): \n",
    "    \n",
    "    if training == True:\n",
    "        # Augment and scale images for training\n",
    "        train_generator = train_datagen.flow_from_dataframe(dataframe =extra_data[0:lines], \n",
    "                directory   = image_dir,\n",
    "                x_col       = \"image_id\" ,\n",
    "                y_col       = \"image_location\",\n",
    "                target_size = (HEIGHT, WIDTH),\n",
    "                batch_size  = BATCH_SIZE,\n",
    "                classes     = CLASS_NAMES,\n",
    "                class_mode  = 'categorical',\n",
    "                shuffle     = True)\n",
    "                #save_to_dir=\"output/\",  if you wanna save the cropped images\n",
    "                #save_prefix=\"\",\n",
    "                #save_format='png')\n",
    "\n",
    "        # Scale images for validation\n",
    "        validation_generator = test_datagen.flow_from_dataframe(dataframe = extra_data[lines:], \n",
    "                directory    = image_dir,\n",
    "                x_col        = \"image_id\",\n",
    "                y_col        = \"image_location\",\n",
    "                target_size  = (HEIGHT, WIDTH),\n",
    "                batch_size   = BATCH_SIZE,\n",
    "                classes      = CLASS_NAMES,\n",
    "                class_mode   = 'categorical',\n",
    "                shuffle      = True)\n",
    "        \n",
    "        return train_generator, validation_generator\n",
    "    \n",
    "    else:\n",
    "        # Scale images for testing, no target provided and returned\n",
    "        test_generator = test_datagen.flow_from_dataframe(dataframe = test_data, \n",
    "                directory   = image_dir,\n",
    "                x_col       = \"image_id\",\n",
    "                target_size = (HEIGHT, WIDTH),\n",
    "                batch_size  = BATCH_SIZE,\n",
    "                class_mode  = None,\n",
    "                shuffle     = False)\n",
    "            \n",
    "        return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093b2b53-dc50-49dd-a4b3-ef596f7b6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "487b6253-dcaa-43f2-bf53-11474fba587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7994 validated image filenames belonging to 3 classes.\n",
      "Found 2664 validated image filenames belonging to 3 classes.\n",
      "Epoch 1/30\n",
      "249/249 [==============================] - 211s 836ms/step - loss: 1.2151 - accuracy: 0.5805 - val_loss: 0.4319 - val_accuracy: 0.6426\n",
      "Epoch 2/30\n",
      "249/249 [==============================] - 216s 866ms/step - loss: 0.4387 - accuracy: 0.6091 - val_loss: 0.3651 - val_accuracy: 0.6505\n",
      "Epoch 3/30\n",
      "249/249 [==============================] - 223s 894ms/step - loss: 0.4330 - accuracy: 0.6074 - val_loss: 0.3619 - val_accuracy: 0.6520\n",
      "Epoch 4/30\n",
      "249/249 [==============================] - 1283s 5s/step - loss: 0.4158 - accuracy: 0.6166 - val_loss: 0.3698 - val_accuracy: 0.6614\n",
      "Epoch 5/30\n",
      "249/249 [==============================] - 548s 2s/step - loss: 0.4175 - accuracy: 0.6188 - val_loss: 0.3763 - val_accuracy: 0.6502\n",
      "Epoch 6/30\n",
      "249/249 [==============================] - 220s 882ms/step - loss: 0.4210 - accuracy: 0.6119 - val_loss: 0.3812 - val_accuracy: 0.6532\n",
      "Epoch 7/30\n",
      "249/249 [==============================] - 241s 968ms/step - loss: 0.4123 - accuracy: 0.6127 - val_loss: 0.3863 - val_accuracy: 0.6430\n",
      "Epoch 8/30\n",
      "249/249 [==============================] - 246s 986ms/step - loss: 0.4127 - accuracy: 0.6144 - val_loss: 0.3758 - val_accuracy: 0.6449\n",
      "Epoch 9/30\n",
      "249/249 [==============================] - 250s 1s/step - loss: 0.4101 - accuracy: 0.6204 - val_loss: 0.3733 - val_accuracy: 0.6599\n",
      "Epoch 10/30\n",
      "249/249 [==============================] - 251s 1s/step - loss: 0.4041 - accuracy: 0.6248 - val_loss: 0.3519 - val_accuracy: 0.6629\n",
      "Epoch 11/30\n",
      "249/249 [==============================] - 243s 974ms/step - loss: 0.4041 - accuracy: 0.6152 - val_loss: 0.3659 - val_accuracy: 0.6603\n",
      "Epoch 12/30\n",
      "249/249 [==============================] - 245s 982ms/step - loss: 0.4068 - accuracy: 0.6153 - val_loss: 0.3693 - val_accuracy: 0.6588\n",
      "Epoch 13/30\n",
      "249/249 [==============================] - 265s 1s/step - loss: 0.4008 - accuracy: 0.6255 - val_loss: 0.3581 - val_accuracy: 0.6607\n",
      "Epoch 14/30\n",
      "249/249 [==============================] - 269s 1s/step - loss: 0.3930 - accuracy: 0.6272 - val_loss: 0.3689 - val_accuracy: 0.6558\n",
      "Epoch 15/30\n",
      "249/249 [==============================] - 261s 1s/step - loss: 0.4022 - accuracy: 0.6247 - val_loss: 0.3592 - val_accuracy: 0.6610\n",
      "Epoch 16/30\n",
      "249/249 [==============================] - 255s 1s/step - loss: 0.3962 - accuracy: 0.6231 - val_loss: 0.3880 - val_accuracy: 0.6456\n",
      "Epoch 17/30\n",
      "249/249 [==============================] - 1205s 5s/step - loss: 0.3999 - accuracy: 0.6218 - val_loss: 0.3585 - val_accuracy: 0.6592\n",
      "Epoch 18/30\n",
      "249/249 [==============================] - 733s 3s/step - loss: 0.3937 - accuracy: 0.6256 - val_loss: 0.3585 - val_accuracy: 0.6456\n",
      "Epoch 19/30\n",
      "249/249 [==============================] - 222s 888ms/step - loss: 0.3955 - accuracy: 0.6230 - val_loss: 0.3804 - val_accuracy: 0.6490\n",
      "Epoch 20/30\n",
      "249/249 [==============================] - 229s 917ms/step - loss: 0.3883 - accuracy: 0.6320 - val_loss: 0.3565 - val_accuracy: 0.6393\n",
      "Epoch 21/30\n",
      "249/249 [==============================] - 1222s 5s/step - loss: 0.3952 - accuracy: 0.6240 - val_loss: 0.3525 - val_accuracy: 0.6629\n",
      "Epoch 22/30\n",
      "249/249 [==============================] - 1330s 5s/step - loss: 0.3967 - accuracy: 0.6292 - val_loss: 0.3490 - val_accuracy: 0.6637\n",
      "Epoch 23/30\n",
      "249/249 [==============================] - 209s 830ms/step - loss: 0.3920 - accuracy: 0.6280 - val_loss: 0.3677 - val_accuracy: 0.6565\n",
      "Epoch 24/30\n",
      "249/249 [==============================] - 219s 878ms/step - loss: 0.3872 - accuracy: 0.6372 - val_loss: 0.3565 - val_accuracy: 0.6295\n",
      "Epoch 25/30\n",
      "249/249 [==============================] - 227s 910ms/step - loss: 0.3983 - accuracy: 0.6132 - val_loss: 0.3584 - val_accuracy: 0.6633\n",
      "Epoch 26/30\n",
      "249/249 [==============================] - 230s 924ms/step - loss: 0.3953 - accuracy: 0.6255 - val_loss: 0.3495 - val_accuracy: 0.6622\n",
      "Epoch 27/30\n",
      "249/249 [==============================] - 1248s 5s/step - loss: 0.3904 - accuracy: 0.6290 - val_loss: 0.3671 - val_accuracy: 0.6580\n",
      "Epoch 28/30\n",
      "249/249 [==============================] - 3078s 12s/step - loss: 0.3911 - accuracy: 0.6299 - val_loss: 0.3729 - val_accuracy: 0.6321\n",
      "Epoch 29/30\n",
      "249/249 [==============================] - 3342s 13s/step - loss: 0.3910 - accuracy: 0.6280 - val_loss: 0.3564 - val_accuracy: 0.6603\n",
      "Epoch 30/30\n",
      "249/249 [==============================] - 1348s 5s/step - loss: 0.3964 - accuracy: 0.6271 - val_loss: 0.3604 - val_accuracy: 0.6291\n"
     ]
    }
   ],
   "source": [
    "#change the last layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(2048, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final softmax layer with 3 nodes for classification output\n",
    "x = layers.Dense(NCLASSES, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = 'accuracy')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "train_datagen, test_datagen = preprocess()\n",
    "train_generator, validation_generator = use_image_generator(train_datagen, test_datagen, training=True)\n",
    "    \n",
    "inception =  model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=TRAINING_SIZE // BATCH_SIZE, \n",
    "        epochs=30,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266b312d-b10e-4f62-9ce4-048be6aa6462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 01:23:16.256450: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: extra_images_location/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('extra_images_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da3eeb-55e8-42d1-9c24-7c006e5351ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
