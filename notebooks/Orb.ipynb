{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ff2a37-4062-4849-b172-8164b2a7fb0f",
   "metadata": {},
   "source": [
    "## Feature Matching using ORB\n",
    "\n",
    "* Oriented FAST and rotated BRIEF (ORB) is a fast robust local feature detector, first presented by Ethan Rublee et al. in 2011,[1] that can be used in computer vision tasks like object recognition or 3D reconstruction. It is based on the FAST keypoint detector and a modified version of the visual descriptor BRIEF (Binary Robust Independent Elementary Features). Its aim is to provide a fast and efficient alternative to SIFT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc48ca-3c36-4d2b-8e91-173be24b2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import cv2 as cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import numpy as nm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02055e7c-f3bd-4345-8aab-2f1bc95b0ed0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Using orb on one image \n",
    "Display Keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dca10-8dea-42af-8a94-6c659b80c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols, rows = 4, 3\n",
    "def grid_display(list_of_images, no_of_columns=2, figsize=(15,15), title = False):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    column = 0\n",
    "    z = 0\n",
    "    for i in range(len(list_of_images)):\n",
    "        column += 1\n",
    "        #  check for end of column and create a new figure\n",
    "        if column == no_of_columns+1:\n",
    "            fig = plt.figure(figsize=figsize)\n",
    "            column = 1\n",
    "        fig.add_subplot(1, no_of_columns, column)\n",
    "        if title:\n",
    "            if i >= no_of_columns:\n",
    "                plt.title(titles[z])\n",
    "                z +=1\n",
    "            else:\n",
    "                plt.title(titles[i])\n",
    "        plt.imshow(list_of_images[i])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a1f40-dff6-488f-8302-3de0584a31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ORB keypoints\n",
    "image_all=[]\n",
    "titles = ['original', 'ORB Detected', \"Zoom Image\"]\n",
    "img = cv2.imread('../images/ID_0A3BSR6Q.jpg', 1)\n",
    "image_all.append(img)\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "# find the keypoints with ORB\n",
    "kp = orb.detect(img,None)\n",
    "# compute the descriptors with ORB\n",
    "kp, des = orb.compute(img, kp)\n",
    "# draw only keypoints location,not size and orientation\n",
    "img2 = cv2.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\n",
    "image_all.append(img2)\n",
    "img3 = img2[350:800,600:1250]\n",
    "image_all.append(img3)\n",
    "grid_display(image_all, 3, (35,35), title = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84862d52-c3fe-4d6f-8aeb-48202ea8cce4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract ORB features from all images\n",
    "\n",
    "Currently disabled due to uncertainity of this method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891b611-6c1b-41bb-936c-9e62ffd9bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction mit ORB\n",
    "# Lets test it with 11 images first\n",
    "'''\n",
    "#read the images\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../test_images/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6a712-7616-4777-8e0c-ab87a11f5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes 8 seconds for 11 images\n",
    "#for all images it would take 31 minutes (2500 images)\n",
    "'''\n",
    "allFeatures=[]\n",
    "\n",
    "for filename in os.listdir('../test_images/'):\n",
    "    img = cv2.imread('../test_images/'+filename)\n",
    "    img1 = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
    "    mask = np.zeros(img1.shape[:2],np.uint8)\n",
    "    bgdModel = np.zeros((1,65),np.float64)\n",
    "    fgdModel = np.zeros((1,65),np.float64)\n",
    "    rect = (5,5,235,235)\n",
    "    cv2.grabCut(img1,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT) #image segmentation\n",
    "    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "    img1 = img1*mask2[:,:,np.newaxis]\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    #img1= cv2.GaussianBlur(img1,(5,5),cv2.BORDER_DEFAULT) \n",
    "    ll=cv2.equalizeHist(img1)\n",
    "    orb = cv2.ORB_create(nfeatures=200,scoreType = cv2.ORB_HARRIS_SCORE)\n",
    "    keypoints, descriptors = orb.detectAndCompute(ll, None) # here we compute the keypoints and descriptor\n",
    "    allFeatures.append(descriptors)#array[ImageNb][FeatureNb]\n",
    "'''   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b63d0-a264-4301-acc8-8e245661902f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2accf9a-e3b3-43e0-99bf-0a14cba2c951",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature Matching using ORB with color-masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b031f-2dd3-4feb-9c82-e0d5de1d1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img = cv2.imread('../images/ID_0A3BSR6Q.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77afdaec-7894-4378-8433-53b4604c9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display an image using matplotlib\n",
    "def show_image(img, title, colorspace):\n",
    "    dpi = 96\n",
    "    figsize = (img.shape[1] / dpi, img.shape[0] / dpi)\n",
    "    #figsize = (224,224)\n",
    "    fig, ax = plt.subplots(figsize = figsize, dpi = dpi)\n",
    "    if colorspace == 'RGB':\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), interpolation = 'spline16')\n",
    "    if colorspace == 'gray':\n",
    "        plt.imshow(img, cmap = 'gray')\n",
    "    plt.title(title, fontsize = 12)\n",
    "    ax.axis('off')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60de8a3-a6bc-4e36-91ac-77ee8407dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(src_img, 'Source image containing one turtle', 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfec18b-8c17-48d0-8039-46502524a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change colorspace from BGR to HSV --> Hue , Saturation, Value (in HSV) --> object tracking based on color\n",
    "src_img_hsv = cv2.cvtColor(src_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define limits of yellow HSV values\n",
    "yellow_lower = np.array([1, 30, 30])\n",
    "yellow_upper = np.array([80, 55, 150])\n",
    "\n",
    "# Filter the image and get the mask\n",
    "mask = cv2.inRange(src_img_hsv, yellow_lower, yellow_upper)\n",
    "\n",
    "show_image(mask, 'Yellow color filter mask', 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c449b-ebf7-4a66-8ece-52ce25c1da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove white noise\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "show_image(opening, 'Morphological opening', 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf9e51-091f-4480-8dbe-ebdf4ea1bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove small black dots\n",
    "closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "show_image(closing, 'Morphological closing', 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57354527-b739-46f6-855f-7a87b77184fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back the fine boundary edges using dilation\n",
    "kernel1 = np.ones((2, 2), np.uint8)\n",
    "dilation = cv2.dilate(closing, kernel1, iterations = 1)\n",
    "\n",
    "show_image(dilation, 'Morphological dilation', 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9060833-46f1-4320-8725-1497ceaead93",
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, _ = cv2.findContours(dilation, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# There are 2 contours: outer one is the rectangle(ish) and inner one is the circle(ish)\n",
    "# Get the outer contour (it has larger area than the inner contour)\n",
    "c1 = max(contours, key = cv2.contourArea)\n",
    "\n",
    "# Define the bounding rectangle around the contour\n",
    "rect = cv2.minAreaRect(c1)\n",
    "\n",
    "# Get the 4 corner coordinates of the rectangle\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "\n",
    "# Draw the bounding rectangle to show the marked object\n",
    "temp_img = src_img.copy()\n",
    "bdg_rect = cv2.drawContours(temp_img, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "show_image(bdg_rect, 'Marked object to be extracted', 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a9857-dbb4-41bd-b1b3-64fcceefcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.boxPoints(rect) returns the coordinates (x, y) as the following list:\n",
    "# [[bottom right], [bottom left], [top left], [top right]]\n",
    "\n",
    "width = box[0][0] - box[1][0]\n",
    "height = box[1][1] - box[2][1]\n",
    "\n",
    "src_pts = box.astype('float32')\n",
    "dst_pts = np.array([[width, height],\n",
    "                    [0, height],\n",
    "                    [0, 0],\n",
    "                    [width, 0]], dtype = 'float32')\n",
    "\n",
    "# Get the transformation matrix\n",
    "M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "\n",
    "# Apply the perspective transformation\n",
    "warped = cv2.warpPerspective(src_img, M, (width, height))\n",
    "\n",
    "# Save it as the query image\n",
    "query_img = warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3be9bf-d219-45ba-a8f1-b1ab72fafede",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(query_img, 'Query image', 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbada0b4-9510-4f1e-ab1a-5e76108efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ORB object\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Detect and visualize the features\n",
    "features = orb.detect(query_img, None)\n",
    "f_img = cv2.drawKeypoints(query_img, features, None, color = (0, 255, 0), flags = 0)\n",
    "\n",
    "show_image(f_img, 'Detected features', 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b74f9a-65fa-4782-83da-c5014a38ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match features and find the object\n",
    "def match_feature_find_object(query_img, train_img, min_matches): \n",
    "    # Create an ORB object\n",
    "    orb = cv2.ORB_create(nfeatures=100000)\n",
    "    \n",
    "    features1, des1 = orb.detectAndCompute(query_img, None)\n",
    "    features2, des2 = orb.detectAndCompute(train_img, None)\n",
    "\n",
    "    # Create Brute-Force matcher object\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "    matches = bf.knnMatch(des1, des2, k = 2)\n",
    "    \n",
    "    # Nearest neighbour ratio test to find good matches\n",
    "    good = []    \n",
    "    good_without_lists = []    \n",
    "    matches = [match for match in matches if len(match) == 2] \n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.8 * n.distance:\n",
    "            good.append([m])\n",
    "            good_without_lists.append(m)\n",
    "         \n",
    "    if len(good) >= min_matches:\n",
    "        # Draw a polygon around the recognized object\n",
    "        src_pts = np.float32([features1[m.queryIdx].pt for m in good_without_lists]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([features2[m.trainIdx].pt for m in good_without_lists]).reshape(-1, 1, 2)\n",
    "        \n",
    "        # Get the transformation matrix\n",
    "        M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "               \n",
    "        # Find the perspective transformation to get the corresponding points\n",
    "        h, w = query_img.shape[:2]\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "        \n",
    "        train_img = cv2.polylines(train_img, [np.int32(dst)], True, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        print('Not enough good matches are found - {}/{}'.format(len(good), min_matches))\n",
    "            \n",
    "    result_img = cv2.drawMatchesKnn(query_img, features1, train_img, features2, good, None, flags = 2)\n",
    "    \n",
    "    show_image(result_img, 'Feature matching and object recognition', 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f696ae-a2d6-4a66-a398-a09356eebf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = cv2.imread('../images/ID_0A3BSR6Q.jpg')\n",
    "match_feature_find_object(query_img, train_img, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f0b17-d400-4c49-8510-5d0d1f968230",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ORB Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59608892-bd75-4379-bc33-b16645b901eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../images/'\n",
    "img_first = cv2.imread(os.path.join(dataset_path, 'ID_0A3BSR6Q.jpg'))\n",
    "img_first = cv2.cvtColor(img_first, cv2.COLOR_BGR2RGB)  # Convert from cv's BRG default color order to RGB\n",
    "\n",
    "orb = cv2.ORB_create()  # OpenCV 3 backward incompatibility: Do not create a detector with `cv2.ORB()`.\n",
    "key_points, description = orb.detectAndCompute(img_first, None)\n",
    "img_first_keypoints = cv2.drawKeypoints(img_first, \n",
    "                                           key_points, \n",
    "                                           img_first, \n",
    "                                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # Draw circles.\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('ORB Interest Points')\n",
    "plt.imshow(img_first_keypoints); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f967c0c-08ff-438e-abcf-c85a4c0f7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_detect_and_compute(detector, img1, img2):\n",
    "    \"\"\"Detect and compute interest points and their descriptors.\"\"\"\n",
    "    img1 = cv2.imread(os.path.join(dataset_path, img1))\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    kp1, des1 = detector.detectAndCompute(img1, None)\n",
    "    \n",
    "    img2 = cv2.imread(os.path.join(dataset_path, img2))\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    kp2, des2 = detector.detectAndCompute(img2, None)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return img1, kp1, des1,img2, kp2, des2\n",
    "    \n",
    "\n",
    "def draw_image_matches(detector, img1, img2, nmatches=50):\n",
    "    \"\"\"Draw ORB feature matches of the given two images.\"\"\"\n",
    "    img1, kp1, des1, img2, kp2, des2 = image_detect_and_compute(detector,img1, img2)\n",
    "    #img2, kp2, des2 = image_detect_and_compute(detector,img2)\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key = lambda x: x.distance) # Sort matches by distance.  Best come first.\n",
    "    print(matches[:5])\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:nmatches], img2, flags=2) # Show top 10 matches\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.title(type(detector))\n",
    "    plt.imshow(img_matches); plt.show()\n",
    "orb = cv2.ORB_create()\n",
    "    \n",
    "#'ID_0A3BSR6Q.jpg'\n",
    "#'ID_0AEH3RAW.jpg'\n",
    "draw_image_matches(orb, 'ID_0AEH3RAW.jpg', 'ID_0A3BSR6Q.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad92e0-d67c-4540-a694-3d09db8ee2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
