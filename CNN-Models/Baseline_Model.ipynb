{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f2f7ed-5ab8-4e96-b203-03aa342d6939",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc11e1a-4f27-4a1d-bc78-572aae762d72",
   "metadata": {},
   "source": [
    "The baseline model is a transfer-learning CNN model using InceptionV3. \n",
    "\n",
    "Input Images: Original images from the training set and the test set. The train/validation split is hard coded using the lines in the training-dataframe.\n",
    "\n",
    "Preprocessing: Scaling. Augmentation can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571556ec-a206-47cf-befc-514e1fe18612",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefda7c4-8129-4780-850e-57dfcf68da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data preparation\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import packages for modeling\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import csv\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a29e7b-88cb-4601-be4c-5747c0d8c6eb",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71443695-1b4f-4158-8b27-a17abe0c9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to images\n",
    "image_dir = \"../images/\"\n",
    "\n",
    "#Load csv-data and append .JPG to image_id to access images using the dataframe\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.image_id = train_data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.image_id = test_data.image_id.apply(lambda x: x.strip()+\".JPG\")\n",
    "\n",
    "#Get unique_turtle_ids from train.csv\n",
    "unique_turtle_ids = list(train_data['turtle_id'].unique())\n",
    "#Add category for new turtle for test set\n",
    "unique_turtle_ids.append(\"new_turtle\")\n",
    "\n",
    "#Get number of images for train/test split\n",
    "split = 0.7\n",
    "lines = round(len(train_data)*split)\n",
    "\n",
    "#We set some parameters for the model\n",
    "HEIGHT = 224 #image height\n",
    "WIDTH = 224 #image width\n",
    "CHANNELS = 3 #image RGB channels\n",
    "CLASS_NAMES = unique_turtle_ids\n",
    "NCLASSES = len(CLASS_NAMES)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 10 * BATCH_SIZE\n",
    "TRAINING_SIZE = lines\n",
    "TRAINING_STEPS = (TRAINING_SIZE // BATCH_SIZE)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfb01f-e6bc-495a-9d60-5744060e4ec5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c5764-8491-4e04-adf6-65df617fbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(augment = True):\n",
    "    '''\n",
    "    Function to create ImageDataGenerator-Object to augment and scale image\n",
    "    input: augment=True\n",
    "    output: train_datagen, test_datagen\n",
    "    If augment is true, augmentation is applied on train_datagen, scaling for test_datagen.\n",
    "    If augment is false, only scaling is applied for both generators.\n",
    "    '''\n",
    "    if augment == True:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                rotation_range     = 40,\n",
    "                width_shift_range  = 0.2,\n",
    "                height_shift_range = 0.2,\n",
    "                # use \"rescale\" to scale array of original image pixel values to be between [0,1] and specify the parameter rescale=1./255.\n",
    "                rescale            = 1./255,\n",
    "                shear_range        = 0.2,\n",
    "                zoom_range         = 0.2,\n",
    "                horizontal_flip    = False,\n",
    "                fill_mode          = 'nearest')\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "    return train_datagen, test_datagen\n",
    "\n",
    "\n",
    "def use_image_generator(train_datagen, test_datagen, training=True): \n",
    "    '''\n",
    "    Function to apply ImageDataGenerator-Object to images for augmentation and scaling\n",
    "    input: \n",
    "        train_datagen as ImageDataGenerator-Object\n",
    "        test_datagen as ImageDataGenerator-Object\n",
    "        training=True\n",
    "    output: train_generator and validation_generator or test_generator\n",
    "    If training is true, train_generator (augmented image and label) and validation_generator (scaled image and label) are returned.\n",
    "    If training is false, test_generator is returned containing scaled image, no label is returned.\n",
    "    '''\n",
    "    if training == True:\n",
    "        # Augment and scale images for training\n",
    "        # This is a generator that will read pictures found in directory, \n",
    "        # and indefinitely generate batches of augmented image data\n",
    "        # flow_from_directory: Takes the path to a directory & generates batches of augmented data.\n",
    "        train_generator = train_datagen.flow_from_dataframe(dataframe =train_data[0:lines], \n",
    "                directory   = image_dir,\n",
    "                x_col       = \"image_id\", #name of column(in dataframe) having file names\n",
    "                y_col       = \"turtle_id\", #name of column(in dataframe) having class/label\n",
    "                target_size = (HEIGHT, WIDTH),\n",
    "                batch_size  = BATCH_SIZE,\n",
    "                classes     = CLASS_NAMES,\n",
    "                class_mode  = 'categorical',\n",
    "                shuffle     = True)\n",
    "\n",
    "        # Scale images for validation\n",
    "        validation_generator = test_datagen.flow_from_dataframe(dataframe = train_data[lines:], \n",
    "                directory    = image_dir,\n",
    "                x_col        = \"image_id\",\n",
    "                y_col        = \"turtle_id\",\n",
    "                target_size  = (HEIGHT, WIDTH),\n",
    "                batch_size   = BATCH_SIZE,\n",
    "                classes      = CLASS_NAMES,\n",
    "                class_mode   = 'categorical',\n",
    "                shuffle      = True)\n",
    "        \n",
    "        return train_generator, validation_generator\n",
    "    \n",
    "    else:\n",
    "        # Scale images for testing, no target provided and returned\n",
    "        test_generator = test_datagen.flow_from_dataframe(dataframe = test_data, \n",
    "                directory   = image_dir,\n",
    "                x_col       = \"image_id\",\n",
    "                target_size = (HEIGHT, WIDTH),\n",
    "                batch_size  = BATCH_SIZE,\n",
    "                class_mode  = None,\n",
    "                shuffle     = False)\n",
    "            \n",
    "        return test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc99096-58f3-4d7c-8573-ec18059d9397",
   "metadata": {},
   "source": [
    "## Baseline-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230ee9e-b452-4873-9460-e7ab83b83e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e337a-7418-45df-9eab-61cf9c348ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the transfer-learning model and freezing the layers.\n",
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "#change the last layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Epochs for fitting the model\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0e33c-8c98-41ce-9d51-57121449217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some layers to be trained for this task\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "#Add a final softmax layer with 101 nodes for classification output\n",
    "x = layers.Dense(NCLASSES, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = tf.keras.metrics.TopKCategoricalAccuracy(k=5))\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#Loading and preprocessing the data \n",
    "train_datagen, test_datagen = preprocess()\n",
    "train_generator, validation_generator = use_image_generator(train_datagen, test_datagen, training=True)\n",
    "\n",
    "#Fitting the model\n",
    "inception =  model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=TRAINING_STEPS, \n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898c793-b6e6-4cd2-ab72-ba2546a6c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "#model.save('InceptionV3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4408e44-2814-4a0f-b9c3-00eca0499262",
   "metadata": {},
   "source": [
    "## Prepare data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852f049-dc2b-44f8-a88e-7f1ed50f9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = use_image_generator(train_datagen, test_datagen, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899c138-15af-48e7-a3e2-bc51986bd5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probabilities for all turtle id's\n",
    "y_preds = model.predict(test_generator)\n",
    "print(y_preds[0])\n",
    "#Get indices from top 5 predictions\n",
    "# Corrected: [:,:-6:-1] instead of [:,-5:]\n",
    "y_preds = np.argsort(y_preds, axis=1)[:,:-6:-1]\n",
    "\n",
    "#Save indices of top 5 predictions as dataframe\n",
    "df = pd.DataFrame(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee399575-0d53-42cf-ad42-d3ecf06e8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame with top 5 predictions in submission form\n",
    "list = []\n",
    "array = []\n",
    "for line in y_preds:\n",
    "    for id in line:\n",
    "        list.append(CLASS_NAMES[id])\n",
    "    array.append(list)\n",
    "    list = []\n",
    "\n",
    "titles = ['prediction1', 'prediction2','prediction3','prediction4','prediction5']\n",
    "\n",
    "submission = pd.DataFrame(array, columns= titles)\n",
    "\n",
    "#Insert image_ids from test_data\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "submission.insert(loc=0, column='image_id', value=test_data['image_id'])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fe61b-e810-4e4e-ad4b-a77def936c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save submission data as CSV\n",
    "submission.to_csv('../data/submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
