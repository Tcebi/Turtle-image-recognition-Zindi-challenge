{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719fc9fa-ca52-432a-9148-b156ccd51600",
   "metadata": {},
   "source": [
    "# Train CNN-Model\n",
    "## Extra images\n",
    "\n",
    "This notebook will train our model based on pictures in our extra_images_sorted folder and sub-folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6f5c8-da50-498a-8464-6e795523f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the modules\n",
    "import keras\n",
    "from keras import models, layers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.activations import relu, softmax\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Activation, Dropout, Dense, Flatten, concatenate\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "sys.modules['Image'] = Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44f886-57db-42cf-a4ff-9d38692cfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras' data generator can be used to pass the images through the convolutional neural network and apply\n",
    "#rotation and zoom transformations to the images. Check https://keras.io/preprocessing/image/ for more transformations\n",
    "\n",
    "train_data = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        rotation_range=40,\n",
    "        zoom_range=0.2,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #shear_range=0.2,\n",
    "        #fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_data.flow_from_directory(\n",
    "        directory=r\"../sorted_extra_images/train\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=100,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a8394-2e5a-41e2-967d-3e196cd8cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the validation data generator\n",
    "val_data = ImageDataGenerator(rescale = 1./255)\n",
    "                                 \n",
    "val_generator = val_data.flow_from_directory(\n",
    "        directory=r\"../sorted_extra_images/val\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=100,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaaf752-6af0-45c5-8f5e-95bff033019b",
   "metadata": {},
   "source": [
    "## Convolutional neural network: \n",
    "\n",
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6bc00-3ca4-43f3-a3f4-bd83576cbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained VGG19 from keras\n",
    "vgg19 = VGG19(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
    "\n",
    "'''\n",
    "#the transfer learned model should be not trainable\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "'''\n",
    "\n",
    "x = vgg19.layers[-1].output\n",
    "#add dropout and the fully connected layer\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "#add a dense layer with a value equal to the number of classes\n",
    "predictors = Dense(2231, activation='softmax')(x)\n",
    "# Create the model\n",
    "vgg19model = Model(vgg19.input, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd00591-13c3-4b97-9f66-4ba12a7d7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define where to save the model after each epoch\n",
    "filepath = \"../models/extra_images_vgg19_trainable2.h5\"\n",
    "# add a critera to save only if there was an improvement in the model comparing\n",
    "# to the previous epoch (in this caset the model is saved if there was a decrease in the loss value)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# stop training if there is no improvement in model for 4 consecutives epochs.\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "callbacks_list = [checkpoint, early_stopping_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1dfd7-208d-4262-a7e9-f7f409551e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "vgg19model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=5e-5),#define the optimizer and the learning rate\n",
    "              metrics=tf.keras.metrics.TopKCategoricalAccuracy(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2d1bd-c3a7-4704-9dcc-52d94fb4e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"../models/extra_images_vgg19_trainable.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6ef6e-4f05-43e7-ba65-666b29f07077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "batch_size=100\n",
    "model_history=model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=8427//batch_size,#number of pictures in training data set divided by the batch size\n",
    "        epochs=25,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps= 2231// batch_size,#number of pictures in validation data set divided by the batch size\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825be70c-9b70-4d0d-a14d-3db320984b28",
   "metadata": {},
   "source": [
    "### A second iteration with smaller learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb23b4-02cb-4ef7-b121-b65da260b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define where to save the model after each epoch\n",
    "filepath = \"../models/Inception_model_lre-6.h5\"\n",
    "# add a critera to save only if there was an improvement in the model comparing\n",
    "# to the previous epoch (in this caset the model is saved if there was a decrease in the loss value)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# stop training if there is no improvement in model for 3 consecutives epochs.\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "callbacks_list = [checkpoint, early_stopping_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdce1d-a8a9-4dc6-9896-a691841ff50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model=load_model(\"../models/Inception_model.h5\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=SGD(lr=1e-6),\n",
    "             metrics=tf.keras.metrics.TopKCategoricalAccuracy(k=5))\n",
    "#train the model\n",
    "batch_size=64\n",
    "model_history_2=model.fit_generator(\n",
    "        train_generator,\n",
    "    #! BEWARE: steps_per_epoch needs to be adapted: containing number of images in train // batch_size\n",
    "        steps_per_epoch=1822//batch_size,\n",
    "        epochs=20,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps= 300// batch_size,\n",
    "        callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83677a6-1860-45d5-b196-5b07f8a44adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of the plots\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "columns = 2\n",
    "rows = 1\n",
    "\n",
    "#plot loss\n",
    "#the accuracy and loss are stored in the \"model_history\"\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.plot(model_history.history['loss']+ model_history_2.history['loss']) #merge the loss from the two training steps\n",
    "plt.plot(model_history.history['val_loss']+ model_history_2.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
