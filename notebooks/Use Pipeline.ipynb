{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e27b1-188a-4afb-98f9-38818e508f23",
   "metadata": {},
   "source": [
    "# Image Modelling Part 2 - Use Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159167b4-2c3d-4c4d-9f75-2cd5462277c6",
   "metadata": {},
   "source": [
    "In the first notebook we used shell commands to prepare and split our data into a train and evaluation set. \n",
    "Furthermore, we defined some functions that will allow us to directly import our pictures and the corresponding class labels and if we want to also augment our data. \n",
    "Now, we will import the functions from the `image_modelling.py` file and use them to facilitate the data preparation step in this notebook. \n",
    "Lastly, we will use Tensorflow and Keras to create and train our neuronal network to identify turtles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac118d51-c16b-4c7f-a6bf-6276f7a7c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasengbring/neuefische/Capstone_Project_Turtle_Recall/.venv/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "# Import required packages \n",
    "import tensorflow as tf\n",
    "import image_modeling   # import image_modeling.py file\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "import csv\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a387f387-ca7d-4679-a05e-f92de2e9177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559448f7-019f-4047-901c-b477cd27e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b1bb25-2f7f-479e-9aeb-fd56f13b9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variables from image_modelling.py file\n",
    "file = open(\"../data/train.csv\")\n",
    "reader = csv.reader(file)\n",
    "\n",
    "HEIGHT = image_modeling.HEIGHT\n",
    "WIDTH = image_modeling.WIDTH\n",
    "NCLASSES = image_modeling.NCLASSES\n",
    "CLASS_NAMES = image_modeling.CLASS_NAMES\n",
    "BATCH_SIZE = image_modeling.BATCH_SIZE\n",
    "TRAINING_SIZE = image_modeling.TRAINING_SIZE\n",
    "TRAINING_STEPS = (TRAINING_SIZE // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b00031-663c-4004-bf86-4b64e6cd6e5a",
   "metadata": {},
   "source": [
    "Double check if the variables now contain the correct values. ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4748727-e7e8-4d76-b934-3cdd218ab775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07961c12-db7b-402e-adfa-5e55ae4cb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "['t_id_VP2NW7aV', 't_id_qZ0iZYsC', 't_id_3b65X5Lw', 't_id_YjXYTCGC', 't_id_d6aYXtor', 't_id_ksTLswDT', 't_id_hRzOoJ2t', 't_id_utw0thCe', 't_id_k1rScFLB', 't_id_n2FBHk6d', 't_id_ZfvZBX4Q', 't_id_G5eoqwD8', 't_id_FBsGDJhU', 't_id_Ts5LyVQz', 't_id_NW7wn8TC', 't_id_JI6ba2Yx', 't_id_ifWwxWF4', 't_id_uIlC9Gfo', 't_id_dVQ4x3wz', 't_id_3K93fQBS', 't_id_IlO9BOKc', 't_id_DPYQnZyv', 't_id_ROFhVsy2', 't_id_BI99coHt', 't_id_GrxmyS59', 't_id_AOWArhGb', 't_id_4XiPKIk7', 't_id_mpuNp8mf', 't_id_stWei2Uq', 't_id_15bo4NKD', 't_id_QqeoI5F3', 't_id_Kf73l69A', 't_id_Kc1tXDbJ', 't_id_2Yn71r7R', 't_id_iZQiE7wb', 't_id_m2JvEcsg', 't_id_a4VYrmyA', 't_id_UVQa4BMz', 't_id_tjWepji1', 't_id_BXWccqAn', 't_id_1KIezxkh', 't_id_e9i3Lbq4', 't_id_bYageLYA', 't_id_8b8sprYe', 't_id_2QmcRkNj', 't_id_9GFmcOd5', 't_id_smNwfXAT', 't_id_hibDzPAP', 't_id_D3kHUEgp', 't_id_B7LaSiac', 't_id_fjHGjp1w', 't_id_gJaKYxBQ', 't_id_72SiiZCp', 't_id_IP1t15lD', 't_id_uJXT7dGu', 't_id_7gFFZy7i', 't_id_87CLFCvE', 't_id_J5dngbNA', 't_id_HcnnlRda', 't_id_WDCMGvI4', 't_id_g9Fz8PH7', 't_id_SwQZGIpa', 't_id_p77GDtzg', 't_id_D0gA44av', 't_id_OqU1NWEA', 't_id_MwnEYfqe', 't_id_HxxqrdTx', 't_id_IP3xtKuX', 't_id_4ZfTUmwL', 't_id_DbmclTcj', 't_id_AMnriNb5', 't_id_fxTQ5vHC', 't_id_J24awAHQ', 't_id_0g31STvR', 't_id_Imm5pnNf', 't_id_WXXcPTSW', 't_id_VFb44eFm', 't_id_uMOOrQu7', 't_id_3fKmnkBS', 't_id_mXD9Bjsb', 't_id_gGQbn7FA', 't_id_G5JLzvai', 't_id_EEbWq5Pj', 't_id_GOIvCduN', 't_id_pCO59rOk', 't_id_iD9ikw6Y', 't_id_dc6pjonY', 't_id_uVgs53Cz', 't_id_C0wevyOl', 't_id_KysHzQQK', 't_id_2E8o5Jtl', 't_id_ip3jsrYo', 't_id_niEvmwXu', 't_id_Lhp87PBX', 't_id_Dv4O8bOM', 't_id_dhdJMT1K', 't_id_X3bKBWYW', 't_id_0DPPpRUz', 't_id_gz3whk0q', 't_id_D7NA71la']\n",
      "100\n",
      "46\n",
      "1502\n"
     ]
    }
   ],
   "source": [
    "# You can compare this output with the variables in the image_modelling.py file...\n",
    "print(HEIGHT)\n",
    "print(CLASS_NAMES)\n",
    "print(NCLASSES)\n",
    "print(TRAINING_STEPS)\n",
    "print(TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bda7a2-6102-4eff-93b9-e9b74e6cd405",
   "metadata": {},
   "source": [
    "## Building our Model\n",
    "\n",
    "Building and training a neural network involves various steps: \n",
    "1. define the architecture of the model\n",
    "2. compile the model\n",
    "3. train the model\n",
    "4. evaluate the model\n",
    "\n",
    "We have to start with defining the architecture. Our neural network will consist of several layers that are chained together. The input layer of our model will take our input data and hand it over to the flatten layer, which is responsible for reformatting our data. It will transform the format of our images from a three-dimensional array (HEIGHT, WIDTH, 3) to a one-dimensional array of size HEIGHT * WIDTH * 3. \n",
    "After the pixels are flattened we use a dense layer that returns a logits array with length `NCLASSES`. Each node in this layer contains a score that indicates the current image belongs to one of the n classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf568e1-d651-47ba-a15f-a4fa901170bc",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26c3f1e-f4cd-4f1e-9abb-4c49c2205ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple linear model.\n",
    "def linear_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Flatten(data_format=\"channels_last\"))\n",
    "    # We want to have a simple linear model so we have \n",
    "    # no activation function. \n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c74046-b4ed-42ec-9816-4ad3f68a74a4",
   "metadata": {},
   "source": [
    "Before we can train our model we need to compile it and define more settings. We have to choose a loss function, an optimizer and metrics. \n",
    "* The **loss function** measures how accurate the model is during training by calculating the model error. Usually we want to minimize this function to improve our model. As you can see in the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses) there are lot's of different loss functions to choose from. Some, e.g. the mean squared errror, hopefully look familiar to you. ;) \n",
    "* The **[optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)** defines how the model is updated based on the data and the loss function. One optimizer we've already covered earlier and which is also used for neural networks is the stochastic gradient descent (SGD) algorithm.   \n",
    "* The **metric** is used to monitor the training process. Here we can choose one of the metrics we've already encountered or [many more](https://www.tensorflow.org/api_docs/python/tf/keras/metrics). \n",
    "\n",
    "The following function compiles our model, loads the data using the `load_dataset()` function from the image_modelling.py file and trains the model on the loaded data. In the end the function returns our fitted model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ea0e7a-60e1-405f-9b16-1fd512965815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model,batch_size=32):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        # The model outputs one-hot-encoded logits, so we need\n",
    "        # use the sparse version of the crossentropy loss.\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    train_datagen, test_datagen = image_modeling.preprocess()\n",
    "    train_generator, validation_generator = image_modeling.generate_augmented_image(train_datagen, test_datagen, augment_randomly=False)\n",
    "    \n",
    "    model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=TRAINING_STEPS, \n",
    "        epochs=10,\n",
    "        callbacks=[tensorboard_callback])\n",
    "          \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2fdebe-9f42-4061-ac71-d5d4ad7f0d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Found 1502 validated image filenames belonging to 100 classes.\n",
      "Found 641 validated image filenames belonging to 100 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 14:52:16.090085: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 37s 792ms/step - loss: 172.5483 - accuracy: 0.0177 - val_loss: 191.6018 - val_accuracy: 0.0094\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 36s 794ms/step - loss: 132.5329 - accuracy: 0.0136 - val_loss: 81.6908 - val_accuracy: 0.0109\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 37s 803ms/step - loss: 67.2809 - accuracy: 0.0156 - val_loss: 77.1141 - val_accuracy: 0.0140\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 37s 799ms/step - loss: 60.3134 - accuracy: 0.0211 - val_loss: 74.4706 - val_accuracy: 0.0172\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 37s 810ms/step - loss: 63.2776 - accuracy: 0.0177 - val_loss: 66.7910 - val_accuracy: 0.0125\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 37s 815ms/step - loss: 63.6632 - accuracy: 0.0286 - val_loss: 67.6307 - val_accuracy: 0.0156\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 37s 805ms/step - loss: 56.1535 - accuracy: 0.0218 - val_loss: 65.6211 - val_accuracy: 0.0125\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 37s 817ms/step - loss: 63.6693 - accuracy: 0.0197 - val_loss: 73.5030 - val_accuracy: 0.0109\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 38s 823ms/step - loss: 65.9627 - accuracy: 0.0163 - val_loss: 64.3725 - val_accuracy: 0.0312\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 38s 829ms/step - loss: 58.1215 - accuracy: 0.0197 - val_loss: 60.7548 - val_accuracy: 0.0172\n"
     ]
    }
   ],
   "source": [
    "# Build and train our model using the prior defined functions \n",
    "model = linear_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56088167-b93d-4639-ac63-40255d0e6ca8",
   "metadata": {},
   "source": [
    "Let us use Tensorboard to monitor our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b04f0c-1e01-4539-8771-ecd56286261b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ea033e0a80ed562a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ea033e0a80ed562a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8a69c-781d-41fc-8a4b-6f2d1b2de1bb",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "Our simple model is not performing well. Maybe we can boost its performance by adding more layers.\n",
    "\n",
    "In the following `dnn_model()` function we add three more hidden, dense layers after the flatten layer to increase our models complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce98cfb8-748e-43d4-bace-3af0733d2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare a neural network with hidden layers to the linear model\n",
    "def dnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Flatten(data_format=\"channels_last\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 40, activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 40, activation = \"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 30, activation = \"relu\"))\n",
    "    # We want to have a simple linear model so we have \n",
    "    # no activation function. \n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97794117-9aee-4fda-a85c-4b513dd9575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 validated image filenames belonging to 100 classes.\n",
      "Found 641 validated image filenames belonging to 100 classes.\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 37s 795ms/step - loss: 9.2400 - accuracy: 0.0116 - val_loss: 4.6004 - val_accuracy: 0.0156\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 36s 775ms/step - loss: 4.5952 - accuracy: 0.0286 - val_loss: 4.5939 - val_accuracy: 0.0140\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 36s 779ms/step - loss: 4.5852 - accuracy: 0.0218 - val_loss: 4.5860 - val_accuracy: 0.0281\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 36s 790ms/step - loss: 4.5720 - accuracy: 0.0245 - val_loss: 4.5764 - val_accuracy: 0.0140\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 37s 802ms/step - loss: 4.5548 - accuracy: 0.0259 - val_loss: 4.5645 - val_accuracy: 0.0281\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 37s 818ms/step - loss: 4.5351 - accuracy: 0.0340 - val_loss: 4.5508 - val_accuracy: 0.0281\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 38s 822ms/step - loss: 4.5116 - accuracy: 0.0320 - val_loss: 4.5380 - val_accuracy: 0.0281\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 38s 830ms/step - loss: 4.4858 - accuracy: 0.0340 - val_loss: 4.5264 - val_accuracy: 0.0281\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 38s 829ms/step - loss: 4.4664 - accuracy: 0.0333 - val_loss: 4.5195 - val_accuracy: 0.0281\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 38s 837ms/step - loss: 4.4498 - accuracy: 0.0340 - val_loss: 4.5164 - val_accuracy: 0.0281\n"
     ]
    }
   ],
   "source": [
    "# Let us fit the deep neural network\n",
    "model = dnn_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0148-7d2d-4a11-9441-65639369aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dc998-8c17-4195-9ea2-016cd783ce14",
   "metadata": {},
   "source": [
    "Adding more hidden layers to our model, did indeed increase the accuracy. But still, the model's performance leaves something to be desired. Since we are working with images, switching to a convolutional neural network might help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f44d66-bc7c-4ea5-9d40-0234126ba2f1",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "CNN's are widely used for image recognition. They are regularized versions of DNN's able to be deeper without generating as much parameters due to its [convolutional](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) and pooling layers.\n",
    "\n",
    "The architecture of our CNN is even more complex. This time we combine dense layers with `Conv2D` and `MaxPooling2D` layers. The convolutional and max pooling layers are inserted between the input layer and the flatten layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db4670c8-ad93-4533-a1da-0c158e9f32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us move on to a CNN model. \n",
    "def cnn_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[HEIGHT, WIDTH, 3], name='image'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=[5, 5], padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=20, kernel_size=[5, 5], padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15784cd7-83e1-450d-a4c4-a1c2d8bfb64d",
   "metadata": {},
   "source": [
    "We can have a look at the architecture of our model with the method `.summary`. As you can see in the summary below, the output of each `Conv2D` and `MaxPooling2D` layer is also a three dimensional tensor of shape (height, width, channels). As we go deeper into the network the dimensions shrink. One advantage of the shrinking dimensions is that we can computationally afford to add more output channels in each convolutional layer. We can control the number of the output channels of those layers with the `filters` argument. \n",
    "\n",
    "However, at the end of our model we still need the combination of the flatten and dense layers to perform classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1231a24d-56dc-4019-aa5a-b0bde1e6f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 10)      760       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 112, 112, 10)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 20)      5020      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 56, 56, 20)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 62720)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 300)               18816300  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,852,180\n",
      "Trainable params: 18,852,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e814fc79-a858-4c48-a582-0ea8996cb4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 validated image filenames belonging to 100 classes.\n",
      "Found 641 validated image filenames belonging to 100 classes.\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 44s 939ms/step - loss: 4.9629 - accuracy: 0.0279 - val_loss: 4.5233 - val_accuracy: 0.0390\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 42s 911ms/step - loss: 4.4841 - accuracy: 0.0333 - val_loss: 4.4995 - val_accuracy: 0.0156\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 41s 896ms/step - loss: 4.4489 - accuracy: 0.0224 - val_loss: 4.5037 - val_accuracy: 0.0156\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 42s 905ms/step - loss: 4.4223 - accuracy: 0.0313 - val_loss: 4.4875 - val_accuracy: 0.0250\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 42s 913ms/step - loss: 4.4292 - accuracy: 0.0354 - val_loss: 4.4964 - val_accuracy: 0.0281\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 42s 915ms/step - loss: 4.3973 - accuracy: 0.0347 - val_loss: 4.4835 - val_accuracy: 0.0421\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 42s 918ms/step - loss: 4.3793 - accuracy: 0.0327 - val_loss: 4.4794 - val_accuracy: 0.0359\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 42s 924ms/step - loss: 4.3705 - accuracy: 0.0333 - val_loss: 4.4856 - val_accuracy: 0.0265\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 42s 923ms/step - loss: 4.3484 - accuracy: 0.0381 - val_loss: 4.5895 - val_accuracy: 0.0328\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 42s 919ms/step - loss: 4.3199 - accuracy: 0.0463 - val_loss: 4.4878 - val_accuracy: 0.0421\n"
     ]
    }
   ],
   "source": [
    "# Let us fit the convolutional neural network\n",
    "model = cnn_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc98fdf-e19e-409a-be68-31814562eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783eea5c-83af-40f1-8410-aebdbdcaa013",
   "metadata": {},
   "source": [
    "We see the CNN give better results than the DNN. But we still have heavy overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f652f9-4e89-4782-a2d5-c3b5c156adf0",
   "metadata": {},
   "source": [
    ">__Exercise__: try to reduce overfitting of your model by: \n",
    "- making full use of your data augmentation by increasing `steps_per_epoch` in the `train_and_evaluate` function.\n",
    "- [adding regularization](https://keras.io/api/layers/regularizers/) to the `Conv2D` and `Dense` layers.\n",
    "- Adding dropout layers. See section CNN Dropout Regularization of [this](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)\n",
    "\n",
    "You can also try adding additional `Conv2D`, `MaxPooling2D` or `Dense` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42e33-a846-43dc-92bb-8a4fd3a5f125",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning is when a model is trained on one task and is then reused for another task. One approach to transfer learning is fine-tuning. Here you take a trained neural net, exchange the last layer (head) for another layer, that fits the new task and then train the weights of the last layer only. \n",
    "\n",
    "First we need to download the headless model (this can take a while) we use MobileNetV2 which is a CNN that was trained on the [ImageNet](https://en.wikipedia.org/wiki/ImageNet) dataset, consisting of over 14 million images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc53795c-03bb-4aae-955c-2ecd4ce3ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,input_shape=(HEIGHT,WIDTH,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d37d23-a8c8-481c-bc07-b24dec923217",
   "metadata": {},
   "source": [
    "We only want to train the last layer therefore we freeze the layers of our headless model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cd5b0b4-53c8-4d0f-9ea0-a3b2fff80779",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ce17-6fe8-4f30-a378-5961b7e714e6",
   "metadata": {},
   "source": [
    "Now we define our model by simply adding the output layer to our pretrained net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c93b6e1-e8ce-4202-8351-94be03fbda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(feature_extractor_layer)\n",
    "    # TODO: add the correct output layer here\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=300, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=NCLASSES, activation=None))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8211eae9-746e-4f39-a10b-d4d802538955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 validated image filenames belonging to 100 classes.\n",
      "Found 641 validated image filenames belonging to 100 classes.\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 54s 1s/step - loss: 4.5754 - accuracy: 0.0469 - val_loss: 4.4644 - val_accuracy: 0.0499\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 50s 1s/step - loss: 4.2323 - accuracy: 0.0823 - val_loss: 4.3220 - val_accuracy: 0.0686\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 51s 1s/step - loss: 4.0292 - accuracy: 0.1136 - val_loss: 4.1266 - val_accuracy: 0.1061\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 52s 1s/step - loss: 3.7459 - accuracy: 0.1449 - val_loss: 4.0281 - val_accuracy: 0.1092\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 52s 1s/step - loss: 3.5448 - accuracy: 0.1925 - val_loss: 3.9804 - val_accuracy: 0.1092\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 51s 1s/step - loss: 3.3544 - accuracy: 0.2048 - val_loss: 3.9418 - val_accuracy: 0.1139\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 53s 1s/step - loss: 3.1785 - accuracy: 0.2514 - val_loss: 3.7692 - val_accuracy: 0.1482\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 57s 1s/step - loss: 3.0009 - accuracy: 0.2803 - val_loss: 3.7543 - val_accuracy: 0.1435\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 58s 1s/step - loss: 2.8741 - accuracy: 0.3102 - val_loss: 3.8137 - val_accuracy: 0.1498\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 57s 1s/step - loss: 2.7793 - accuracy: 0.3238 - val_loss: 3.6646 - val_accuracy: 0.1716\n"
     ]
    }
   ],
   "source": [
    "# Let us fit our transfer learning model\n",
    "model = transfer_learning_model()\n",
    "trained_model = train_and_evaluate(model, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2b3232d-a28c-411b-bcb3-afe050c5a7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6822), started 0:24:32 ago. (Use '!kill 6822' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f66cfec643039d9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f66cfec643039d9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d255ce-6a71-49a7-a0d0-48412f4e9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9b069-d2b8-4a31-bc4c-3b66550b9479",
   "metadata": {},
   "source": [
    "As we see the results of fine-tuning surpass the results of the linear model, DNN and CNN. Fine-tuning is a very powerful approach which can generalize well even with limited amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3caebcfe-6dfa-4006-8ec0-18534b1f3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 7s 0us/step\n",
      "87924736/87910968 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d6c337c-9153-4cfb-98ac-c27094d4ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f949222-5f28-4d0b-a151-72d4ba7882ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tobiasengbring/neuefische/Capstone_Project_Turtle_Recall/.venv/lib/python3.9/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1502 validated image filenames belonging to 100 classes.\n",
      "Found 641 validated image filenames belonging to 100 classes.\n",
      "Epoch 1/10\n",
      "31/31 [==============================] - 48s 1s/step - loss: 0.5512 - acc: 0.9663 - val_loss: 0.0823 - val_acc: 0.9885\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3194 - acc: 0.9781 - val_loss: 0.4556 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 47s 2s/step - loss: 0.2966 - acc: 0.9841 - val_loss: 0.4546 - val_acc: 0.8509\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 48s 2s/step - loss: 0.3254 - acc: 0.9752 - val_loss: 0.4544 - val_acc: 0.9900\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 50s 2s/step - loss: 0.2790 - acc: 0.9801 - val_loss: 0.3687 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 51s 2s/step - loss: 0.1955 - acc: 0.9860 - val_loss: 0.1795 - val_acc: 0.9579\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 51s 2s/step - loss: 0.2497 - acc: 0.9781 - val_loss: 0.2967 - val_acc: 0.9900\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 51s 2s/step - loss: 0.1690 - acc: 0.9880 - val_loss: 0.1377 - val_acc: 0.9854\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 51s 2s/step - loss: 0.2165 - acc: 0.9821 - val_loss: 0.1049 - val_acc: 0.9808\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.1841 - acc: 0.9860 - val_loss: 0.1520 - val_acc: 0.9655\n"
     ]
    }
   ],
   "source": [
    "#change the last layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = 'acc')\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "train_datagen, test_datagen = image_modeling.preprocess()\n",
    "train_generator, validation_generator = image_modeling.generate_augmented_image(train_datagen, test_datagen, augment_randomly=False)\n",
    "    \n",
    "inception =  model.fit(\n",
    "        train_generator, \n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=1000 // 32, \n",
    "        epochs=10,\n",
    "        callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0fa7d27-ab89-405c-95f4-0cabe34ea0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6822), started 0:45:42 ago. (Use '!kill 6822' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b3bd26aa5e192dd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b3bd26aa5e192dd3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6de906a-997f-4905-b37a-1e6b16ab7153",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m array, label \u001b[38;5;241m=\u001b[39m train_generator\n\u001b[1;32m      2\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(train_generator)\n\u001b[1;32m      3\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(y_preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array, label = train_generator\n",
    "y_preds = model.predict(train_generator)\n",
    "y_preds = np.argsort(y_preds, axis=1)[:,-5:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
