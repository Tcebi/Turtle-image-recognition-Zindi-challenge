{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f1139-e183-475c-ae8d-61a2897e514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "import shutil\n",
    "from imutils import paths\n",
    "from random import shuffle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a6c72-215d-4438-b64c-ab227420b478",
   "metadata": {},
   "source": [
    "## Sort train images into train and validation folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd1b0f-676f-41ad-85d7-4d165fb5ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_corrected.csv')\n",
    "train_data.image_id = train_data.image_id.apply(lambda x: x.strip()+\".jpg\")\n",
    "train_data['turtle_id-image_id'] = train_data.turtle_id + \"-\" + train_data.image_id\n",
    "train_data['turtle_id-image_location'] = train_data.turtle_id + \"-\" + train_data.image_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d1a2c-96a0-4419-a743-ae9848365bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b574b-e894-433a-9860-a7551aa8d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all pictures in the image_dir\n",
    "image_dir = '../images/'\n",
    "imagePaths = sorted(list(paths.list_images(image_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e895be3-fd96-4662-aa80-6ec5695b793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all the turtle_ids in train_data\n",
    "turtle_ids = train_data['turtle_id']\n",
    "turtle_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b70d75-e294-4ec2-a361-d280fca5da91",
   "metadata": {},
   "source": [
    "## Sort images\n",
    "To sort the images you need to create a subfolder \"sorted_images\" into the main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26a674-4bad-469d-aecc-938a6dfc0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after listing all individuals create two empty folders for each individual \n",
    "#one for the training and another for the validaiton dataset\n",
    "\n",
    "#define the folder were the training and validation datasets will be placed\n",
    "if not os.path.exists(\"../sorted_images\"):\n",
    "    os.makedirs(\"../sorted_images\")\n",
    "root_dir=\"../sorted_images\"\n",
    "\n",
    "#loop through all individuals and create a folder for the training dataset\n",
    "# and a folder for the validation dataset\n",
    "for i in range(0, len(turtle_ids)):\n",
    "    train_dir=root_dir+\"/train/\"+turtle_ids[i]#variable with the full path of the training folder\n",
    "    val_dir=root_dir+\"/val/\"+turtle_ids[i]#variable with the full path of the validation folder\n",
    "    if not os.path.exists(train_dir):#condition for if the folder already exists\n",
    "        os.makedirs(train_dir)#create the folder\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0bc9e9-a3e5-449d-bf20-8ce312df8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines will create a folder called \"new_turtle\" into our train and validation sub-folders. \n",
    "#This is necessary for our specific task. (Reminder: if the image most likelx does not belong to any turtle_id, the models needs to output \"new_turtle\")\n",
    "#These folders will not contain any pictures\n",
    "#os.makedirs(\"../sorted_images/train/new_turtle\")\n",
    "#os.makedirs(\"../sorted_images/val/new_turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a27e5-d06b-456b-a0a4-c2ee9da7af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check length of train_data, i.e. number of relevant pictures\n",
    "round(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fa194-42dc-4bd7-a454-dcf7b5fff1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this example we are going to select 1822 (approx 86%) pictures for training and 300 (approx 14%) pictures for validation.\n",
    "#We setup the condition that the validation pictures should contain at least one picture per turtle_id for each image_location\n",
    "#as per now we have nothing to avoid having pictures that are very similiar in both datasets, which could result in overfitting the CNN\n",
    "\n",
    "#define the number of validation pictures and the number of training pictures\n",
    "N_val_pics=300\n",
    "N_train_pics=1822\n",
    "\n",
    "#create two empty lists to store the pictures files that are going to be moved to the training \n",
    "#and validation fodlers\n",
    "training_pictures=[]\n",
    "validation_pictures=[]\n",
    "\n",
    "#list to check if combination is already used\n",
    "turtle_id_image_location=[]\n",
    "\n",
    "#loop through each individual turtle_id and secondary image_location\n",
    "for index in range(0, len(train_data)):\n",
    "    if train_data['turtle_id-image_location'][index] not in turtle_id_image_location:\n",
    "        validation_pictures.append(train_data['turtle_id-image_id'][index])\n",
    "        turtle_id_image_location.append(train_data['turtle_id-image_location'][index])\n",
    "    else:\n",
    "        training_pictures.append(train_data['turtle_id-image_id'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cf98a-d8d6-41f0-9173-76548ee0aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if number of validation pictures is indeed 300\n",
    "len(validation_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75d9e1-a0fd-4721-83fc-ae3f4fac24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92407290-a1af-43f1-ae76-402df33094f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "train_images = []\n",
    "#loop through the list of pictures\n",
    "#move the pictures files to the validation folder\n",
    "for i in range(0, len(validation_pictures)):\n",
    "    #get the picture name\n",
    "    val_file_name = image_dir + validation_pictures[i].split('-')[-1]\n",
    "    image_name = validation_pictures[i].split('-')[-1]\n",
    "\n",
    "    #create a variable with the directory and the name of the pictures file\n",
    "    output_name_val=root_dir+\"/val/\"+validation_pictures[i].split('-')[-0]+\"/\"+image_name\n",
    "       \n",
    "    #move the file\n",
    "    shutil.copy(val_file_name, output_name_val)\n",
    "    \n",
    "    # put the information into a DataFrame\n",
    "    val_images.append(image_name)\n",
    "\n",
    "for i in range(0, len(training_pictures)):\n",
    "    #get the picture name (e.g. \"01103F7D5A_2018-11-26_07-56-03.jpg\")\n",
    "    train_file_name = image_dir + training_pictures[i].split('-')[-1]\n",
    "    image_name = training_pictures[i].split('-')[-1]\n",
    "\n",
    "    #create a variable with the directory and the name of the pictures file\n",
    "    output_name_train=root_dir+\"/train/\"+training_pictures[i].split('-')[-0]+\"/\"+image_name\n",
    "       \n",
    "    #move the file\n",
    "    shutil.copy(train_file_name, output_name_train)    \n",
    "    \n",
    "    train_images.append(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b472e06-4099-44d5-8c53-56de0a599dd3",
   "metadata": {},
   "source": [
    "## Sort test images\n",
    "To appriately use our Train_CNN pipeline, we need to move our test pictures into a subfolder, as well.\n",
    "\n",
    "As we don't know the turtle_id for these pictures, we will save them into subfolders containing their image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b25d0-f941-4726-aa75-a202ce4c644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_corrected.csv')\n",
    "test_data_jpg = pd.read_csv('../data/test_corrected.csv')\n",
    "test_data_jpg.image_id = test_data_jpg.image_id.apply(lambda x: x.strip()+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72716717-17e0-4dd2-bf56-e2177761f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the folder were the training and validation datasets will be placed\n",
    "root_dir=\"../sorted_images/\"\n",
    "image_dir=\"../images\"\n",
    "#loop through all individuals and create a folder for the test dataset\n",
    "for i in range(0, len(test_data)):\n",
    "    test_dir_folder= root_dir + \"test/\" + test_data['image_id'][i]#variable with the full path of the training folder\n",
    "    if not os.path.exists(test_dir_folder):#condition for if the folder already exists\n",
    "        os.makedirs(test_dir_folder)#create the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951475d-a48a-4d67-9f84-661bd285a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put images into the folders\n",
    "\n",
    "root_dir=\"../sorted_images/\"\n",
    "image_dir=\"../images/\"\n",
    "\n",
    "for i in range(0, len(test_data)):    \n",
    "    image_name = test_data_jpg['image_id'][i]\n",
    "    test_file_name = image_dir + image_name\n",
    "    output_name = root_dir + \"test/\" + test_data['image_id'][i] + \"/\" + image_name\n",
    "    #move the file\n",
    "    shutil.copy(test_file_name, output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919454d5-e8cc-4f97-a8de-efb339033b90",
   "metadata": {},
   "source": [
    "## Create dataframes with new folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f6ebce-b817-492d-9a34-47f7b02907ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cd805-e33f-4ca7-b3d5-a592f6712109",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "train_dir=\"../sorted_images/train/\"\n",
    "\n",
    "for r, d, f in os.walk(train_dir):\n",
    "    d.sort()\n",
    "    for file in sorted(f):\n",
    "        if \".jpg\" in file:\n",
    "            train.append((d,os.path.join(r,file)))\n",
    "\n",
    "df_train = pd.DataFrame(train,columns=['folder','image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0500a-7599-45a9-aa1e-6ef24c73737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "train_dir=\"../sorted_images/train/\"\n",
    "\n",
    "for r, d, f in os.walk(train_dir):\n",
    "    d.sort(key=str.lower)\n",
    "    for file in sorted(f):\n",
    "        if \".jpg\" in file:\n",
    "            train.append((d,os.path.join(r,file)))\n",
    "\n",
    "df_train = pd.DataFrame(train,columns=['folder','image_id'])\n",
    "\n",
    "val = []\n",
    "val_dir=\"../sorted_images/val/\"\n",
    "\n",
    "for r, d, f in os.walk(val_dir):\n",
    "    d.sort(key=str.lower)\n",
    "    for file in sorted(f):\n",
    "        if \".jpg\" in file:\n",
    "            val.append((d,os.path.join(r,file)))\n",
    "\n",
    "df_val = pd.DataFrame(val,columns=['folder','image_id'])\n",
    "\n",
    "test = []\n",
    "val_dir=\"../sorted_images/test/\"\n",
    "\n",
    "for r, d, f in os.walk(val_dir):\n",
    "    d.sort(key=str.lower)\n",
    "    for file in sorted(f):\n",
    "        if \".jpg\" in file:\n",
    "            test.append((d,os.path.join(r,file)))\n",
    "\n",
    "df_test = pd.DataFrame(test,columns=['folder','image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bb9a7-ae03-466f-a21d-aa2f8097ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train['image_id']\n",
    "df_val = df_val['image_id']\n",
    "df_test = df_test['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c391b2f-86ed-4886-9827-7292073a5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ddfc5-eeac-42ad-b7e2-dd1b38df0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for line in range(len(df_train)):\n",
    "    train.append(df_train[line][-15:])\n",
    "\n",
    "val = []\n",
    "for line in range(len(df_val)):\n",
    "    val.append(df_val[line][-15:])\n",
    "\n",
    "test = []\n",
    "for line in range(len(df_test)):\n",
    "    test.append(df_test[line][-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fa2d4-4901-4852-b982-b7ab46f5aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3872f90-4dcc-4fd6-a54a-256b1eb60336",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_location_train = []\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train_data)):\n",
    "        if train[i] in train_data['image_id'][j]: \n",
    "            image_location_train.append(train_data['image_location'][j])\n",
    "            \n",
    "d_train = {'image_id':train,'image_location':image_location_train}   \n",
    "df_train = pd.DataFrame(d_train)\n",
    "df_train.to_csv('../data/df_sorted_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde54d88-e905-4336-9c4d-42c47e5bf23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_location_val = []\n",
    "for i in range(len(val)):\n",
    "    for j in range(len(train_data)):\n",
    "        if val[i] in train_data['image_id'][j]: \n",
    "            image_location_val.append(train_data['image_location'][j])\n",
    "            \n",
    "d_val = {'image_id':val,'image_location':image_location_val}   \n",
    "df_val = pd.DataFrame(d_val)\n",
    "df_val.to_csv('../data/df_sorted_val.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9a020-f4dc-40ac-b8f8-46f48e4aab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_location_test = []\n",
    "for i in range(len(test)):\n",
    "    for j in range(len(test_data_jpg)):\n",
    "        if test[i] in test_data_jpg['image_id'][j]: \n",
    "            image_location_test.append(test_data_jpg['image_location'][j])\n",
    "            \n",
    "d_test = {'image_id':test,'image_location':image_location_test}   \n",
    "df_test = pd.DataFrame(d_test)\n",
    "df_test.to_csv('../data/df_sorted_test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
